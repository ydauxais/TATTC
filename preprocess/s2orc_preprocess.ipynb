{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsumption class (to avoid pickle errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsumption:\n",
    "    def __init__(self, data, topics) -> None:\n",
    "        self.data_path = data\n",
    "        self.topics_path = topics\n",
    "        self.is_topic_path = True\n",
    "        self.topics_label = \"\"\n",
    "        self.overlaps = None\n",
    "        self.weights = None\n",
    "        self.features = None\n",
    "        self.ifeatures = None\n",
    "        self.lengths = None\n",
    "\n",
    "    def load_data(self):\n",
    "        if os.path.exists(self.data_path):\n",
    "            logging.info('loading preprocessed data from %s' % self.data_path)\n",
    "            if self.data_path.endswith(\".txt\"):\n",
    "                self.data = open(self.data_path, \"r\")\n",
    "            else:\n",
    "                with open(self.data_path, 'rb') as fin:\n",
    "                    self.data = pickle.load(fin)\n",
    "        else:\n",
    "            logging.error(\"preprocessed data doesn't exist\")\n",
    "            sys.exit()\n",
    "\n",
    "    def load_topics(self):\n",
    "        fname = self.topics_path\n",
    "        if not os.path.exists(fname):\n",
    "            self.is_topic_path = False\n",
    "            fname = '/calcul/datasets/nasa/topics-%s.txt' % self.topics_path\n",
    "            if not os.path.exists(fname):\n",
    "                logging.error(\"not a filename or a valid topic name\")\n",
    "                sys.exit()\n",
    "        logging.info('loading topics from %s' % fname)\n",
    "        with open(fname, 'r') as f_in:\n",
    "            self.topics = f_in.read()\n",
    "        self.topics = self.topics.split('\\n')\n",
    "        logging.info('loaded %d topics' % len(self.topics))\n",
    "\n",
    "    def make_counts(self):\n",
    "        logging.info(\"getting topics counts\")\n",
    "        pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "        self.vectorizer = CountVectorizer(vocabulary=set(\n",
    "            self.topics), token_pattern=pattern, ngram_range=(1, 3))\n",
    "        self.counts = self.vectorizer.transform(self.data)\n",
    "        if isinstance(self.data, io.IOBase):\n",
    "            self.data.close()\n",
    "        del(self.data)\n",
    "        self.features = self.vectorizer.get_feature_names()\n",
    "        self.ifeatures = {k: v for v, k in enumerate(self.features)}\n",
    "\n",
    "    def make_matrices(self):\n",
    "        logging.info(\"getting the overlap and weight matrices\")\n",
    "        self.counts = binarize(self.counts)\n",
    "        self.overlaps = self.counts.T.dot(self.counts)\n",
    "        # del(self.counts)\n",
    "        self.overlaps.data *= self.overlaps.data > 1\n",
    "        self.overlaps.eliminate_zeros()\n",
    "        self.lengths = self.overlaps.diagonal()\n",
    "        diagonal = sp.diags([1./x if x > 0 else 0 for x in self.lengths])\n",
    "        self.overlaps = diagonal.dot(self.overlaps)\n",
    "\n",
    "        self.weights = self.overlaps.minimum(self.overlaps.T)\n",
    "        dotp_sub = self.overlaps - self.weights\n",
    "        dotp_sub.eliminate_zeros()\n",
    "        dotp_sub.data[dotp_sub.data > 0] = 1\n",
    "        self.weights = self.weights.minimum(dotp_sub)\n",
    "        self.weights.data *= -1\n",
    "\n",
    "    def dump(self, obj, prefix, suffix):\n",
    "        filename = prefix + \"/\" + \\\n",
    "            self.data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        if self.is_topic_path:\n",
    "            if self.topics_label:\n",
    "                filename += \"-\" + self.topics_label\n",
    "        else:\n",
    "            filename += \"-\" + self.topics_path \n",
    "        filename += suffix\n",
    "        with open(filename, \"wb\") as fout:\n",
    "            pickle.dump(obj, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract S2ORC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(t):\n",
    "    temp = str.lower(t)\n",
    "    tokens = [\".\", \",\", \":\", \"(\", \")\", \";\", \"!\", \"[\", \"]\", \"/\"]\n",
    "    for token in tokens:\n",
    "        temp = temp.replace(token, \"\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/calcul/datasets/s2orc/20200705v1/full/metadata/\"\n",
    "with open(root + \"title_abstract_processed.txt\", \"w\") as fout:    \n",
    "    with open(root + \"metadata_processed.txt\", \"w\") as fout2:\n",
    "        for archive in os.listdir(root):\n",
    "            if archive.endswith(\".gz\"):\n",
    "                with gzip.open(root + archive, 'rb') as fin:\n",
    "                    for line in fin:\n",
    "                        paper = json.loads(line.decode('utf-8'))\n",
    "                        if paper[\"abstract\"] is not None:\n",
    "                            fout.write(normalize_text(paper['title']) + \" \" + normalize_text(paper['abstract']) + \"\\n\")\n",
    "                            fout2.write(json.dumps({\"paper_id\" : paper[\"paper_id\"], \n",
    "                                         \"authors\" : paper[\"authors\"], \n",
    "                                         \"venue\" : paper[\"venue\"], \n",
    "                                         \"journal\" : paper[\"journal\"], \n",
    "                                         \"mag_id\" : paper[\"mag_id\"], \n",
    "                                         \"mag_field\" : paper[\"mag_field_of_study\"]}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/!\\ Can only be run after the generation of the subsumption pickle through `knowledge/subsumption.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsumption = pickle.load(open(\"your _subsumption.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsumption = normalize(normalize(subsumption.overlaps, axis=1).tocsc().T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_similarities = n_subsumption.dot(n_subsumption.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(topic_similarities, open(\"topic_similarities.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors from S2ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author:\n",
    "    def __init__(self, d) -> None:\n",
    "        self.first = d[\"first\"]\n",
    "        self.middle = tuple(d[\"middle\"])\n",
    "        self.last = d[\"last\"]\n",
    "        self.suffix = d[\"suffix\"]\n",
    "        self.hash = self.compute_hash()\n",
    "        self.str_hash = \"a\" + str(self.hash)\n",
    "\n",
    "    def compute_hash(self) -> int:\n",
    "        return hash((self.first, self.middle, self.last, self.suffix))\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return isinstance(other, Author) and hash(other) == self.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = {}\n",
    "count = 0\n",
    "with open(root + \"metadata_processed.txt\", \"r\") as fin:\n",
    "    for line in tqdm(fin, total=76556428):\n",
    "        paper = json.loads(line)\n",
    "        for author in paper[\"authors\"]:\n",
    "            a = Author(author)\n",
    "            if a not in authors:\n",
    "                authors[a] = count\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_matrix = dok_matrix((76556428,count), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root + \"metadata_processed.txt\", \"r\") as fin:\n",
    "    for i, line in enumerate(tqdm(fin, total=76556428)):\n",
    "        paper = json.loads(line)\n",
    "        for author in paper[\"authors\"]:\n",
    "            author_matrix[i,authors[Author(author)]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(authors, open(\"author_dict.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(author_matrix, open(\"dok_author_matrix.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_author_matrix = author_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(csr_author_matrix, open(\"csr_author_matrix\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsumption = pickle.load(open(\"your _subsumption.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topics = csr_author_matrix.T.dot(subsumption.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(author_matrix, open(\"author_topics.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic by author subsumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topics = pickle.load(open(\"your author_topics.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlaps = binarize(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlaps = author_overlaps.T.dot(author_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlaps.data *= author_overlaps.data > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlaps.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = author_overlaps.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal = sp.diags([1./x if x > 0 else 0 for x in lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlaps = diagonal.dot(author_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = author_overlaps.minimum(author_overlaps.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub = author_overlaps - weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub.data[dotp_sub.data > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.minimum(dotp_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.data *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weights, open(\"topic_from_author_subsumptions.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic by author similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topics = pickle.load(open(\"author_topics.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topics = normalize(author_topics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topics = normalize(author_topics.tocsc().T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please verify that threshold and chunk_size parameters fit your setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "threshold = 0.1 \n",
    "threads = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_linear_dot(i):\n",
    "        start = chunk_size * i\n",
    "        chunk = author_topics[start:int(min(\n",
    "            start+chunk_size, float(author_topics.shape[0])))] * author_topics.T\n",
    "        return chunk.multiply(chunk > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPool(threads)\n",
    "chunks = pool.map(chunk_linear_dot, range(\n",
    "    int(math.ceil(float(author_topics.shape[0]) / chunk_size))))\n",
    "similarities = vstack(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(similarities, open(\"your _author_similarities.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields of study from S2ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {}\n",
    "index = 0\n",
    "with open(\"/calcul/datasets/s2orc/20200705v1/full/metadata/metadata_processed.txt\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        paper = json.loads(line)\n",
    "        if paper[\"mag_field\"] is not None:\n",
    "            for field in paper[\"mag_field\"]:\n",
    "                if field not in fields:\n",
    "                    fields[field] = index\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_matrix = dok_matrix((76556428,len(fields)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/calcul/datasets/s2orc/20200705v1/full/metadata/metadata_processed.txt\", \"r\") as fin:\n",
    "    for i, line in enumerate(tqdm(fin, total=76556428)):\n",
    "        paper = json.loads(line)\n",
    "        if paper[\"mag_field\"] is not None:\n",
    "            for field in paper[\"mag_field\"]:\n",
    "                field_matrix[i,fields[field]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_field_matrix = field_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(csr_field_matrix, open(\"field_matrix.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_topics = csr_field_matrix.T.dot(subsumption.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(field_topics, open(\"field_topics.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic by field similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_topics = pickle.load(open(\"field_topics.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fields = normalize(normalize(field_topics, axis=1).tocsc().T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = n_fields.dot(n_fields.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(similarities, open(\"field_similarities.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic by field subsumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_overlaps = binarize(field_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_overlaps = field_overlaps.T.dot(field_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_overlaps.data *= field_overlaps.data > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_overlaps.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = field_overlaps.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal = sp.diags([1./x if x > 0 else 0 for x in lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_overlaps = diagonal.dot(field_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = field_overlaps.minimum(field_overlaps.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub = field_overlaps - weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotp_sub.data[dotp_sub.data > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.minimum(dotp_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.data *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weights, open(\"field_subsumptions.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_YDA",
   "language": "python",
   "name": "py38_yda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
