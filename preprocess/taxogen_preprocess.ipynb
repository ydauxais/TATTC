{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsumption class (to avoid pickle errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import binarize\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsumption:\n",
    "    def __init__(self, data, topics) -> None:\n",
    "        self.data_path = data\n",
    "        self.topics_path = topics\n",
    "        self.is_topic_path = True\n",
    "        self.topics_label = \"\"\n",
    "        self.overlaps = None\n",
    "        self.weights = None\n",
    "        self.features = None\n",
    "        self.ifeatures = None\n",
    "        self.lengths = None\n",
    "\n",
    "    def load_data(self):\n",
    "        if os.path.exists(self.data_path):\n",
    "            logging.info('loading preprocessed data from %s' % self.data_path)\n",
    "            if self.data_path.endswith(\".txt\"):\n",
    "                self.data = open(self.data_path, \"r\")\n",
    "            else:\n",
    "                with open(self.data_path, 'rb') as fin:\n",
    "                    self.data = pickle.load(fin)\n",
    "        else:\n",
    "            logging.error(\"preprocessed data doesn't exist\")\n",
    "            sys.exit()\n",
    "\n",
    "    def load_topics(self):\n",
    "        fname = self.topics_path\n",
    "        if not os.path.exists(fname):\n",
    "            self.is_topic_path = False\n",
    "            fname = '/calcul/datasets/nasa/topics-%s.txt' % self.topics_path\n",
    "            if not os.path.exists(fname):\n",
    "                logging.error(\"not a filename or a valid topic name\")\n",
    "                sys.exit()\n",
    "        logging.info('loading topics from %s' % fname)\n",
    "        with open(fname, 'r') as f_in:\n",
    "            self.topics = f_in.read()\n",
    "        self.topics = self.topics.split('\\n')\n",
    "        logging.info('loaded %d topics' % len(self.topics))\n",
    "\n",
    "    def make_counts(self):\n",
    "        logging.info(\"getting topics counts\")\n",
    "        pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "        self.vectorizer = CountVectorizer(vocabulary=set(\n",
    "            self.topics), token_pattern=pattern, ngram_range=(1, 3))\n",
    "        self.counts = self.vectorizer.transform(self.data)\n",
    "        if isinstance(self.data, io.IOBase):\n",
    "            self.data.close()\n",
    "        del(self.data)\n",
    "        self.features = self.vectorizer.get_feature_names()\n",
    "        self.ifeatures = {k: v for v, k in enumerate(self.features)}\n",
    "\n",
    "    def make_matrices(self):\n",
    "        logging.info(\"getting the overlap and weight matrices\")\n",
    "        self.counts = binarize(self.counts)\n",
    "        self.overlaps = self.counts.T.dot(self.counts)\n",
    "        # del(self.counts)\n",
    "        self.overlaps.data *= self.overlaps.data > 1\n",
    "        self.overlaps.eliminate_zeros()\n",
    "        self.lengths = self.overlaps.diagonal()\n",
    "        diagonal = sp.diags([1./x if x > 0 else 0 for x in self.lengths])\n",
    "        self.overlaps = diagonal.dot(self.overlaps)\n",
    "\n",
    "        self.weights = self.overlaps.minimum(self.overlaps.T)\n",
    "        dotp_sub = self.overlaps - self.weights\n",
    "        dotp_sub.eliminate_zeros()\n",
    "        dotp_sub.data[dotp_sub.data > 0] = 1\n",
    "        self.weights = self.weights.minimum(dotp_sub)\n",
    "        self.weights.data *= -1\n",
    "\n",
    "    def dump(self, obj, prefix, suffix):\n",
    "        filename = prefix + \"/\" + \\\n",
    "            self.data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        if self.is_topic_path:\n",
    "            if self.topics_label:\n",
    "                filename += \"-\" + self.topics_label\n",
    "        else:\n",
    "            filename += \"-\" + self.topics_path \n",
    "        filename += suffix\n",
    "        with open(filename, \"wb\") as fout:\n",
    "            pickle.dump(obj, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxogen input file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.load('../data/embeddings/better_europa_embeddings.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.txt\", \"w\") as fout:\n",
    "    with open(\"embeddings.txt\", \"w\") as fout2:\n",
    "        for x in embeddings:\n",
    "            keyword = \"_\".join(x.split())\n",
    "            fout.write(keyword + \"\\n\")\n",
    "            fout2.write(keyword + \" \" + \" \".join([str(e) for e in embeddings[x]]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsumption = pickle.load(open(\"your _subsumption.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers.txt\", \"w\") as fout:\n",
    "    papers = np.unique(subsumption.counts.nonzero()[0])\n",
    "    for paper in papers:\n",
    "        topics = subsumption.counts[paper,:].nonzero()[1]\n",
    "        fout.write(\" \".join([\"_\".join(subsumption.features[topic].split()) for topic in topics]) + \"\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doc_ids.txt\", \"w\") as fout:\n",
    "    papers = np.unique(subsumption.counts.nonzero()[0])\n",
    "    i = 0\n",
    "    for paper in papers:\n",
    "        fout.write(str(i) + \"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keyword_cnt.txt\", \"w\") as fout:\n",
    "    papers = np.unique(subsumption.counts.nonzero()[0])\n",
    "    i = 0\n",
    "    for paper in papers:\n",
    "        l = str(i)\n",
    "        topics = subsumption.counts[paper,:].nonzero()[1]\n",
    "        for topic in topics:\n",
    "            l += \"\\t\" + \"_\".join(subsumption.features[topic].split()) + \"\\t\" + \"1\"\n",
    "        fout.write(l + \"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_index = np.unique(subsumption.counts.nonzero()[0])\n",
    "paper_dict = { paper_index[i]: i for i in range(len(paper_index)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.txt\", \"w\") as fout:\n",
    "    topics = np.unique(subsumption.counts.nonzero()[1])\n",
    "    for topic in topics:\n",
    "        l = \"_\".join(subsumption.features[topic].split())\n",
    "        papers = subsumption.counts[:,topic].nonzero()[0]\n",
    "        l += \"\\t\" + \",\".join([str(paper_dict[paper]) for paper in papers])\n",
    "        fout.write(l + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_YDA",
   "language": "python",
   "name": "py38_yda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
