{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsumption class (to avoid pickle errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import binarize\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsumption:\n",
    "    def __init__(self, data, topics) -> None:\n",
    "        self.data_path = data\n",
    "        self.topics_path = topics\n",
    "        self.is_topic_path = True\n",
    "        self.topics_label = \"\"\n",
    "        self.overlaps = None\n",
    "        self.weights = None\n",
    "        self.features = None\n",
    "        self.ifeatures = None\n",
    "        self.lengths = None\n",
    "\n",
    "    def load_data(self):\n",
    "        if os.path.exists(self.data_path):\n",
    "            logging.info('loading preprocessed data from %s' % self.data_path)\n",
    "            if self.data_path.endswith(\".txt\"):\n",
    "                self.data = open(self.data_path, \"r\")\n",
    "            else:\n",
    "                with open(self.data_path, 'rb') as fin:\n",
    "                    self.data = pickle.load(fin)\n",
    "        else:\n",
    "            logging.error(\"preprocessed data doesn't exist\")\n",
    "            sys.exit()\n",
    "\n",
    "    def load_topics(self):\n",
    "        fname = self.topics_path\n",
    "        if not os.path.exists(fname):\n",
    "            self.is_topic_path = False\n",
    "            fname = '/calcul/datasets/nasa/topics-%s.txt' % self.topics_path\n",
    "            if not os.path.exists(fname):\n",
    "                logging.error(\"not a filename or a valid topic name\")\n",
    "                sys.exit()\n",
    "        logging.info('loading topics from %s' % fname)\n",
    "        with open(fname, 'r') as f_in:\n",
    "            self.topics = f_in.read()\n",
    "        self.topics = self.topics.split('\\n')\n",
    "        logging.info('loaded %d topics' % len(self.topics))\n",
    "\n",
    "    def make_counts(self):\n",
    "        logging.info(\"getting topics counts\")\n",
    "        pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "        self.vectorizer = CountVectorizer(vocabulary=set(\n",
    "            self.topics), token_pattern=pattern, ngram_range=(1, 3))\n",
    "        self.counts = self.vectorizer.transform(self.data)\n",
    "        if isinstance(self.data, io.IOBase):\n",
    "            self.data.close()\n",
    "        del(self.data)\n",
    "        self.features = self.vectorizer.get_feature_names()\n",
    "        self.ifeatures = {k: v for v, k in enumerate(self.features)}\n",
    "\n",
    "    def make_matrices(self):\n",
    "        logging.info(\"getting the overlap and weight matrices\")\n",
    "        self.counts = binarize(self.counts)\n",
    "        self.overlaps = self.counts.T.dot(self.counts)\n",
    "        # del(self.counts)\n",
    "        self.overlaps.data *= self.overlaps.data > 1\n",
    "        self.overlaps.eliminate_zeros()\n",
    "        self.lengths = self.overlaps.diagonal()\n",
    "        diagonal = sp.diags([1./x if x > 0 else 0 for x in self.lengths])\n",
    "        self.overlaps = diagonal.dot(self.overlaps)\n",
    "\n",
    "        self.weights = self.overlaps.minimum(self.overlaps.T)\n",
    "        dotp_sub = self.overlaps - self.weights\n",
    "        dotp_sub.eliminate_zeros()\n",
    "        dotp_sub.data[dotp_sub.data > 0] = 1\n",
    "        self.weights = self.weights.minimum(dotp_sub)\n",
    "        self.weights.data *= -1\n",
    "\n",
    "    def dump(self, obj, prefix, suffix):\n",
    "        filename = prefix + \"/\" + \\\n",
    "            self.data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        if self.is_topic_path:\n",
    "            if self.topics_label:\n",
    "                filename += \"-\" + self.topics_label\n",
    "        else:\n",
    "            filename += \"-\" + self.topics_path \n",
    "        filename += suffix\n",
    "        with open(filename, \"wb\") as fout:\n",
    "            pickle.dump(obj, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsumption = pickle.load(\n",
    "    open(\"your _subsumption.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(subsumption.features, open(\"your _features.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(subsumption.ifeatures, open(\"your _ifeatures.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(subsumption.overlaps, open(\"your _overlaps.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(subsumption.weights, open(\"your _weights.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_YDA",
   "language": "python",
   "name": "py38_yda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
